{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Start with our import of Tensorflow\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Fashion MNIST dataset is available directly in the tf.keras datasets API.\n",
    "# Load it like this:-\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling load_data() on this object will give us two sets of two lists, these will be training and testing values\n",
    "# for the graphics that contains the handwritten digits and their labels.\n",
    "\n",
    "(train_images,train_labels), (test_images,test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   2   0   2   1   0   0   0  31  90   0\n",
      "    0   0   1   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   2   0   0   0 128  82  61 210 126\n",
      "    0   0   0   0   1   0  37  42   3   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   2 115 165 163 194 177 154 163\n",
      "  170   0   0   0   0  37  79  55  36   0]\n",
      " [  0   0   0   0   0   0   0   0   0  57 172 172 170 178 161 177 191 190\n",
      "  206  86   0   0   0  37   0  12  41   0]\n",
      " [  1   0   0   2   4   0   0   0 106 177 145 162 131 170 221 228 206 225\n",
      "  200 195 175 133 127 115  22  93 193   0]\n",
      " [  1   1   0   0   0   0   0  83 154 160 173 171 222 238 212 162 139 127\n",
      "  135 149 180 208 206 203 196 197 213  36]\n",
      " [  0   0   0   0  26  66 152 170 172 184 200 209 186 155 158 171 195 201\n",
      "  173 150 157 143 145 179 212 214 225  43]\n",
      " [  0  78 137 144 167 166 152 168 150 138 153 151 150 167 186 181 175 188\n",
      "  210 195 171 172 208 222 219 215 233  60]\n",
      " [ 58 149 137 136 115 122 145 151 147 153 153 168 171 174 164 175 194 185\n",
      "  188 186 152 196 228 223 224 232 237  63]\n",
      " [144 174 167 174 170 157 135 146 153 163 162 152 161 173 181 189 201 217\n",
      "  227 193 213 255 236 229 217 211 197   0]\n",
      " [ 73 163 122 151 185 214 221 225 228 227 225 225 227 231 232 231 203 172\n",
      "  158 128 107 104  78  46  25   2  22   0]\n",
      " [  0  99 148  89  64  53  53  71  93  95  95  95  82  64  43  24  20  18\n",
      "   25  26  31  40  45  66  88 105 148  86]\n",
      " [  0   0   0  70 101 131 130 109  91  91 107 112 107 119 110 100 104 111\n",
      "  106  99  88  83  82  80  77  59   6   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAQiklEQVR4nO3db2yd5XnH8d9lx44T559NSEiTjNCQboWJP5sXKLCOjhYBexGqTqx0QpnEFDSViU5MGmKTyt6haS3ai6prGFHTqaNjooy8QG1DhIa6QYZBaUiAQEiTJsGJSRwSxwEntq+98ENlwPf1mPN/3N+PZNl+Lj/nuXJOfj7H536e+zZ3F4BPvrZmNwCgMQg7kAnCDmSCsAOZIOxAJmY18mCdNtu71N3IQwJZeU8jOuujNl2tqrCb2U2S/klSu6R/cfcHo5/vUreushuqOSSAwHbflqxV/DLezNolfUfSzZIukXS7mV1S6e0BqK9q/mZfK2mvu+9z97OSfiRpXW3aAlBr1YR9uaSDU74/VGz7ADPbYGb9ZtZ/TqNVHA5ANer+bry7b3T3Pnfv69Dseh8OQEI1YT8saeWU71cU2wC0oGrC/oKkNWZ2kZl1SvqqpC21aQtArVU89ObuY2Z2t6SfanLobZO7765ZZwBqqqpxdnd/StJTNeoFQB1xuiyQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5kg7EAmCDuQCcIOZKKqJZvNbL+kYUnjksbcva8WTQGovarCXviCux+rwe0AqCNexgOZqDbsLulnZvaimW2Y7gfMbIOZ9ZtZ/zmNVnk4AJWq9mX8de5+2MyWSNpqZq+5+7NTf8DdN0raKEkLrNerPB6AClX1zO7uh4vPg5KekLS2Fk0BqL2Kw25m3WY2//2vJd0oaVetGgNQW9W8jF8q6Qkze/92/s3df1KTrgDUXMVhd/d9ki6vYS8A6oihNyAThB3IBGEHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiATtZhwEmXa2uP6xHh1+/tEUKvv5EDtl3wmrA98YXGytuQ7/1Prdj5o8vLr6dX5fmlFPLMDmSDsQCYIO5AJwg5kgrADmSDsQCYIO5AJxtkbodpx9LL9q7D3oavDes+uYKxaUvdg3NvX/uKnydr3Pv2lcN/V9z4f1ktVM5YejdHP4Lbtykvj3WenH3PrfyXed2wsrKfwzA5kgrADmSDsQCYIO5AJwg5kgrADmSDsQCbMG3hd7wLr9avshoYd7xOjZBx+/9+vTdbmXnYi3Hfc4/Hkq5b9KqzvGz4vrB98uydZ65pzNtx3bue5sL7wlr1hvZ7KxtGHLl8Q1rsH0v+2rqNnwn0ndqTH4bf7Np3yoWkf1NJndjPbZGaDZrZryrZeM9tqZm8Un9OPKICWMJOX8d+XdNOHtt0naZu7r5G0rfgeQAsrDbu7Pytp6EOb10naXHy9WdKtNe4LQI1Vem78UncfKL4+Imlp6gfNbIOkDZLUpbkVHg5Atap+N94n3+FLvsvn7hvdvc/d+zo0u9rDAahQpWE/ambLJKn4PFi7lgDUQ6Vh3yJpffH1eklP1qYdAPVS+je7mT0q6XpJi83skKRvSnpQ0mNmdqekA5Juq2eTn3SjN/9eWD93z/GwvlhHk7Xxifj3+fzZo2H9ucOrwvrSBcNh/epVv0zWJjzubU13/IJxy11/ENYXf++5sF5PPd+Pj93ekx6tHl+zotbtSJpB2N399kSJs2OA/0c4XRbIBGEHMkHYgUwQdiAThB3IROOnkq7XMroll4Fae1z3c/HlltU4dP81Yf2CPzwU1g8cPD+sz1v0brI2cnB+uK8ujof1/u7Sp8J6b/vpsP7cyJpk7cq5+8N9/+vUb4X1vjt3hPU9X1mVrP3q9eQZ3pIkOxtf+jtrJK5fPLQyrB9el67/7td2hvseimf/TuKZHcgEYQcyQdiBTBB2IBOEHcgEYQcyQdiBTDR+Kum2L1Z+Aw3s9eM4/uefC+unboinBu7ojJfg7elOj6NL8XTQf7X66XDfSzuPhPWtI58N64+8Hp9DcHqwO1lrG4nPfZjomgjrbfPjqabXLE9fIvv5xfE01Oc87m10Ij5F5Z2xeAq2tvTkTvrNufFj8tSfXpusPf/awzp55q3KppIG8MlA2IFMEHYgE4QdyARhBzJB2IFMEHYgE42/nr1JY+XtF18U1k9dviSsH/lKesrlxYveDvc9vy0eLz5/zkhYXz0vvv0lnenpnB87Gk9TvfPwp8J6e3v8eC2aF59DcM2V6amkF8x6L9z3ucH4MRs4tjCs7z/Wm6zt+eXvh/uqLf53f+Y30tN3S9LNF+wK6x02nqyt7IjnGBif05GseVv6nAue2YFMEHYgE4QdyARhBzJB2IFMEHYgE4QdyETjx9kDJ9bH14W//bn02KQ647Hs3vNPhXX3d8K6Ts9JlsqWRX57KJ67/a3Ti8P67gXLwroH17OXndawaGE8xn/RoqGwfmasM6zveSc9P/sF3fFjsm7FL8L63t743Iidx9PnELQtiP/df/Sp3WH94q74mvP3JtJj4ZJ0ZmJ2srakPV4Ge/S89L4+q4pxdjPbZGaDZrZryrYHzOywme0oPm4pux0AzTWTl/Hfl3TTNNsfcvcrio942RAATVcadnd/VlL8Wg5Ay6vmDbq7zWxn8TK/J/VDZrbBzPrNrP+c0ueXA6ivSsP+XUmrJV0haUDSt1I/6O4b3b3P3fs6lH5jAUB9VRR2dz/q7uPuPiHpYUlra9sWgFqrKOxmNnUs6MuS4uv5ADRd6Ti7mT0q6XpJi83skKRvSrrezK6Q5JL2S7prJgfzhXM1el36+urjX4yvb9ZIMHY5Gv/eOr3jvLA+EQ8Xa3xJev32EyfTc6NLko+XrPW9oLq14XuCsfJZ7cG5CZI6Sq61f+N4vDZ8z9x4Tvs1C9PX4h8fjedWf/HkhWF9rOz8hhPp8xtuXPNauG/f3H1hfXgifd6FJC1oj/8vr+5Mz2k/ty2eD9+D1AanXJSH3d1vn2bzI2X7AWgtnC4LZIKwA5kg7EAmCDuQCcIOZKKhl7ie7ZEO/HH6msuVS+LLTEfOpofeujvj4YqzF8ZL8L4TXMIqSWPH0vWJeGRN7Z3x8NecufFpxNElrJJ04lR6CKurK75fFpUMnS1feDLevzPef1FHeqrpqxe8GR+740RY/49j8TTZ91z+TLL2lz0Hwn13jMaPSbvFQ5b7z8ZDlvtG05fn7ns33nfOf/5vstbm6fubZ3YgE4QdyARhBzJB2IFMEHYgE4QdyARhBzLR0HH2WadMFzydPuQ7t3WF+7+7Z1GydnJFfEnhZSsPhfU7Vm0P6+e1n07W5rbFY7JdFo91T5T8zj3r8TkCkc5gaWCpvPd2xePJ4yW9/+TkZcnapgPXhPsODceXDp89FNefO5Y+9j/H/yx1vxXPwb3wzfj8gs634nNGxvYfTNZmrYynyJbS+0Z4ZgcyQdiBTBB2IBOEHcgEYQcyQdiBTBB2IBPmZWv61tAC6/Wr7IZk/a2/jsddRy5Lj6UvfD4eo583UDKl8vBYWI+8t7hked4l8e9Ur+Ov3I7h+PGdMxQPOM85Ep+/0HGkZKnrc8H9Oh4/JpoTP6beVjKRQGf6cZnojlcnmuiMz204c0G8f9ljProo3fu7n43v857/Th97z+MP6czgwWlvnGd2IBOEHcgEYQcyQdiBTBB2IBOEHcgEYQcy0VLj7NUY/pOr4/qF8e+1kRUlY77B3O1dx+Lbnh1Pf15qdGFc92CYv+RSeo3Nix//9ndLlpuOL+vWRDBjQtky2SUrF6utZKXrjpH0v63stlUSi5Jp49VWdgrB8fT5B3O2vRzuO/Feehx+u2/TKR+qbJzdzFaa2TNm9oqZ7Taze4rtvWa21czeKD73lN0WgOaZycv4MUn3uvslkq6W9HUzu0TSfZK2ufsaSduK7wG0qNKwu/uAu79UfD0s6VVJyyWtk7S5+LHNkm6tV5MAqvex5qAzs1WSrpS0XdJSdx8oSkckLU3ss0HSBknqUnpNMgD1NeN3481snqTHJX3D3U9Nrfnku3zTvqXh7hvdvc/d+zoUXzwAoH5mFHYz69Bk0H/o7j8uNh81s2VFfZmkwfq0CKAWSl/Gm5lJekTSq+7+7SmlLZLWS3qw+PxkXTqcofn//nxcb1AfQLVKRvUqNpO/2a+VdIekl81sR7Htfk2G/DEzu1PSAUm31adFALVQGnZ3/7mk1JkV9TlDBkDNcboskAnCDmSCsAOZIOxAJgg7kAnCDmSCsAOZIOxAJgg7kAnCDmSCsAOZIOxAJgg7kAnCDmSCsAOZIOxAJgg7kAnCDmSCsAOZIOxAJgg7kAnCDmSCsAOZIOxAJgg7kAnCDmSCsAOZIOxAJgg7kInSsJvZSjN7xsxeMbPdZnZPsf0BMztsZjuKj1vq3y6ASs1kffYxSfe6+0tmNl/Si2a2tag95O7/WL/2ANTKTNZnH5A0UHw9bGavSlpe78YA1NbH+pvdzFZJulLS9mLT3Wa208w2mVlPYp8NZtZvZv3nNFpVswAqN+Owm9k8SY9L+oa7n5L0XUmrJV2hyWf+b023n7tvdPc+d+/r0OwatAygEjMKu5l1aDLoP3T3H0uSux9193F3n5D0sKS19WsTQLVm8m68SXpE0qvu/u0p25dN+bEvS9pV+/YA1MpM3o2/VtIdkl42sx3Ftvsl3W5mV0hySfsl3VWXDgHUxEzejf+5JJum9FTt2wFQL5xBB2SCsAOZIOxAJgg7kAnCDmSCsAOZIOxAJgg7kAnCDmSCsAOZIOxAJgg7kAnCDmSCsAOZMHdv3MHM3pZ0YMqmxZKONayBj6dVe2vVviR6q1Qte7vQ3c+frtDQsH/k4Gb97t7XtAYCrdpbq/Yl0VulGtUbL+OBTBB2IBPNDvvGJh8/0qq9tWpfEr1VqiG9NfVvdgCN0+xndgANQtiBTDQl7GZ2k5ntMbO9ZnZfM3pIMbP9ZvZysQx1f5N72WRmg2a2a8q2XjPbamZvFJ+nXWOvSb21xDLewTLjTb3vmr38ecP/ZjezdkmvS/qSpEOSXpB0u7u/0tBGEsxsv6Q+d2/6CRhm9nlJpyX9wN1/u9j2D5KG3P3B4hdlj7v/TYv09oCk081exrtYrWjZ1GXGJd0q6c/UxPsu6Os2NeB+a8Yz+1pJe919n7uflfQjSeua0EfLc/dnJQ19aPM6SZuLrzdr8j9LwyV6awnuPuDuLxVfD0t6f5nxpt53QV8N0YywL5d0cMr3h9Ra6727pJ+Z2YtmtqHZzUxjqbsPFF8fkbS0mc1Mo3QZ70b60DLjLXPfVbL8ebV4g+6jrnP335F0s6SvFy9XW5JP/g3WSmOnM1rGu1GmWWb815p531W6/Hm1mhH2w5JWTvl+RbGtJbj74eLzoKQn1HpLUR99fwXd4vNgk/v5tVZaxnu6ZcbVAvddM5c/b0bYX5C0xswuMrNOSV+VtKUJfXyEmXUXb5zIzLol3ajWW4p6i6T1xdfrJT3ZxF4+oFWW8U4tM64m33dNX/7c3Rv+IekWTb4j/6akv21GD4m+Pi3pF8XH7mb3JulRTb6sO6fJ9zbulHSepG2S3pD0tKTeFurtXyW9LGmnJoO1rEm9XafJl+g7Je0oPm5p9n0X9NWQ+43TZYFM8AYdkAnCDmSCsAOZIOxAJgg7kAnCDmSCsAOZ+D+KN/v79FCl1gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see how does these values look like ?\n",
    "# We can also experiment and see different images in the array. Try out yourself... \n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_images[52])\n",
    "print(train_labels[52])\n",
    "print(train_images[52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll notice that all the values are between 0 and 255. If we are training a neural network, for various reasons\n",
    "# it's easier if we treat all the values between 0 and 1 (a process called normalization), so we'll do exactly the same\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now design the model\n",
    "\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                tf.keras.layers.Dense(128, activation = tf.nn.relu),\n",
    "                                   tf.keras.layers.Dense(10, activation = tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 2s 838us/step - loss: 0.5004\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 2s 842us/step - loss: 0.3793\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 2s 839us/step - loss: 0.3383\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 1s 794us/step - loss: 0.3133\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 1s 775us/step - loss: 0.2976\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 1s 778us/step - loss: 0.2823\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 1s 779us/step - loss: 0.2697\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 1s 779us/step - loss: 0.2594\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 1s 779us/step - loss: 0.2511\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 2s 816us/step - loss: 0.2423\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 1s 785us/step - loss: 0.2339\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 1s 777us/step - loss: 0.2250\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 1s 778us/step - loss: 0.2185\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 1s 782us/step - loss: 0.2123\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 2s 821us/step - loss: 0.2077\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 1s 769us/step - loss: 0.2013\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 1s 775us/step - loss: 0.1961\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 2s 800us/step - loss: 0.1911\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 2s 837us/step - loss: 0.1862\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 2s 859us/step - loss: 0.1839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcff700c2e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we have to simply build the model. This is done by compiling it with an optimizer and a loss function\n",
    "# and then we train it by calling 'model.fit()' asking it to fit our training data to our training labels\n",
    "# This will take around a few seconds depending upon the number of epochs you're taking.\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy')\n",
    "model.fit(x = train_images, y = train_labels, epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once it's done, we see that our neural network gives a loss of about '0.2'. This tells us that the network gives an accuracy of about 80%. Not great but nor Bad considering the network was only trained for 10 epochs and done quite quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But, how would it work with an unseen data ? Let's find out !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 691us/step - loss: 0.3470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.34697356820106506"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As expected, the model gave an accuracy for about 66% i.e it didn't do well on the unseen data as it did on the training data. Now, try this out for yourself and see that by tweaking the parameters in the neural network or changing the number of epochs, if there's a way for you to get it above '66%' . Good luck with that, until next time... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
